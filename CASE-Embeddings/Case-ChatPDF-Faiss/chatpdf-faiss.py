from PyPDF2 import PdfReader
from langchain_classic.chains.question_answering import load_qa_chain
from langchain_community.callbacks.manager import get_openai_callback
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import FAISS
from typing import List, Tuple
import os
import pickle

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œç¡®ä¿ç›¸å¯¹è·¯å¾„æ­£ç¡®
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())

DASHSCOPE_API_KEY = os.getenv('DASHSCOPE_API_KEY')
if not DASHSCOPE_API_KEY:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")

def extract_text_with_page_numbers(pdf) -> Tuple[str, List[int]]:
    """
    ä»PDFä¸­æå–æ–‡æœ¬å¹¶è®°å½•æ¯è¡Œæ–‡æœ¬å¯¹åº”çš„é¡µç 
    
    å‚æ•°:
        pdf: PDFæ–‡ä»¶å¯¹è±¡
    
    è¿”å›:
        text: æå–çš„æ–‡æœ¬å†…å®¹
        page_numbers: æ¯è¡Œæ–‡æœ¬å¯¹åº”çš„é¡µç åˆ—è¡¨
    """
    text = ""
    page_numbers = []

    for page_number, page in enumerate(pdf.pages, start=1):
        extracted_text = page.extract_text()
        if extracted_text:
            text += extracted_text
            page_numbers.extend([page_number] * len(extracted_text.split("\n")))

    return text, page_numbers

def process_text_with_splitter(text: str, page_numbers: List[int], save_path: str = None) -> FAISS:
    """
    å¤„ç†æ–‡æœ¬å¹¶åˆ›å»ºå‘é‡å­˜å‚¨
    
    å‚æ•°:
        text: æå–çš„æ–‡æœ¬å†…å®¹
        page_numbers: æ¯è¡Œæ–‡æœ¬å¯¹åº”çš„é¡µç åˆ—è¡¨
        save_path: å¯é€‰ï¼Œä¿å­˜å‘é‡æ•°æ®åº“çš„è·¯å¾„
    
    è¿”å›:
        knowledgeBase: åŸºäºFAISSçš„å‘é‡å­˜å‚¨å¯¹è±¡
    """
    # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨ï¼Œç”¨äºå°†é•¿æ–‡æœ¬åˆ†å‰²æˆå°å—
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", ".", " ", ""],
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )

    # åˆ†å‰²æ–‡æœ¬
    chunks = text_splitter.split_text(text)
    print(f"æ–‡æœ¬è¢«åˆ†å‰²æˆ {len(chunks)} ä¸ªå—ã€‚")
        
    # åˆ›å»ºåµŒå…¥æ¨¡å‹
    embeddings = DashScopeEmbeddings(
        model="text-embedding-v1",
        dashscope_api_key=DASHSCOPE_API_KEY,
    )
    
    # ä»æ–‡æœ¬å—åˆ›å»ºçŸ¥è¯†åº“
    knowledgeBase = FAISS.from_texts(chunks, embeddings)
    print("å·²ä»æ–‡æœ¬å—åˆ›å»ºçŸ¥è¯†åº“ã€‚")
    
    # æ”¹è¿›ï¼šå­˜å‚¨æ¯ä¸ªæ–‡æœ¬å—å¯¹åº”çš„é¡µç ä¿¡æ¯
    # åˆ›å»ºåŸå§‹æ–‡æœ¬çš„è¡Œåˆ—è¡¨å’Œå¯¹åº”çš„é¡µç åˆ—è¡¨
    lines = text.split("\n")
    
    # ä¸ºæ¯ä¸ªchunkæ‰¾åˆ°æœ€åŒ¹é…çš„é¡µç 
    page_info = {}
    for chunk in chunks:
        # æŸ¥æ‰¾chunkåœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å¼€å§‹ä½ç½®
        start_idx = text.find(chunk[:100])  # ä½¿ç”¨chunkçš„å‰100ä¸ªå­—ç¬¦ä½œä¸ºå®šä½ç‚¹
        if start_idx == -1:
            # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…ï¼Œåˆ™ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…
            for i, line in enumerate(lines):
                if chunk.startswith(line[:min(50, len(line))]):
                    start_idx = i
                    break
            
            # å¦‚æœä»ç„¶æ‰¾ä¸åˆ°ï¼Œå°è¯•å¦ä¸€ç§åŒ¹é…æ–¹å¼
            if start_idx == -1:
                for i, line in enumerate(lines):
                    if line and line in chunk:
                        start_idx = text.find(line)
                        break
        
        # å¦‚æœæ‰¾åˆ°äº†èµ·å§‹ä½ç½®ï¼Œç¡®å®šå¯¹åº”çš„é¡µç 
        if start_idx != -1:
            # è®¡ç®—è¿™ä¸ªä½ç½®å¯¹åº”åŸæ–‡ä¸­çš„å“ªä¸€è¡Œ
            line_count = text[:start_idx].count("\n")
            # ç¡®ä¿ä¸è¶…å‡ºé¡µç åˆ—è¡¨é•¿åº¦
            if line_count < len(page_numbers):
                page_info[chunk] = page_numbers[line_count]
            else:
                # å¦‚æœè¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªé¡µç 
                page_info[chunk] = page_numbers[-1] if page_numbers else 1
        else:
            # å¦‚æœæ— æ³•åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤é¡µç -1ï¼ˆè¿™é‡Œåº”è¯¥æ ¹æ®å®é™…æƒ…å†µè®¾ç½®ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼ï¼‰
            page_info[chunk] = -1
    
    knowledgeBase.page_info = page_info
    
    # å¦‚æœæä¾›äº†ä¿å­˜è·¯å¾„ï¼Œåˆ™ä¿å­˜å‘é‡æ•°æ®åº“å’Œé¡µç ä¿¡æ¯
    if save_path:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(save_path, exist_ok=True)
        
        # ä¿å­˜FAISSå‘é‡æ•°æ®åº“
        knowledgeBase.save_local(save_path)
        print(f"å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {save_path}")
        
        # ä¿å­˜é¡µç ä¿¡æ¯åˆ°åŒä¸€ç›®å½•
        with open(os.path.join(save_path, "page_info.pkl"), "wb") as f:
            pickle.dump(page_info, f)
        print(f"é¡µç ä¿¡æ¯å·²ä¿å­˜åˆ°: {os.path.join(save_path, 'page_info.pkl')}")

    return knowledgeBase

def load_knowledge_base(load_path: str, embeddings = None) -> FAISS:
    """
    ä»ç£ç›˜åŠ è½½å‘é‡æ•°æ®åº“å’Œé¡µç ä¿¡æ¯
    
    å‚æ•°:
        load_path: å‘é‡æ•°æ®åº“çš„ä¿å­˜è·¯å¾„
        embeddings: å¯é€‰ï¼ŒåµŒå…¥æ¨¡å‹ã€‚å¦‚æœä¸ºNoneï¼Œå°†åˆ›å»ºä¸€ä¸ªæ–°çš„DashScopeEmbeddingså®ä¾‹
    
    è¿”å›:
        knowledgeBase: åŠ è½½çš„FAISSå‘é‡æ•°æ®åº“å¯¹è±¡
    """
    # å¦‚æœæ²¡æœ‰æä¾›åµŒå…¥æ¨¡å‹ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„
    if embeddings is None:
        embeddings = DashScopeEmbeddings(
            model="text-embedding-v1",
            dashscope_api_key=DASHSCOPE_API_KEY,
        )
    
    # åŠ è½½FAISSå‘é‡æ•°æ®åº“ï¼Œæ·»åŠ allow_dangerous_deserialization=Trueå‚æ•°ä»¥å…è®¸ååºåˆ—åŒ–
    knowledgeBase = FAISS.load_local(load_path, embeddings, allow_dangerous_deserialization=True)
    print(f"å‘é‡æ•°æ®åº“å·²ä» {load_path} åŠ è½½ã€‚")
    
    # åŠ è½½é¡µç ä¿¡æ¯
    page_info_path = os.path.join(load_path, "page_info.pkl")
    if os.path.exists(page_info_path):
        with open(page_info_path, "rb") as f:
            page_info = pickle.load(f)
        knowledgeBase.page_info = page_info
        print("é¡µç ä¿¡æ¯å·²åŠ è½½ã€‚")
    else:
        print("è­¦å‘Š: æœªæ‰¾åˆ°é¡µç ä¿¡æ¯æ–‡ä»¶ã€‚")
    
    return knowledgeBase

# è¯»å–PDFæ–‡ä»¶
pdf_reader = PdfReader(os.path.join(SCRIPT_DIR, 'æµ¦å‘ä¸Šæµ·æµ¦ä¸œå‘å±•é“¶è¡Œè¥¿å®‰åˆ†è¡Œä¸ªé‡‘å®¢æˆ·ç»ç†è€ƒæ ¸åŠæ³•.pdf'))
# æå–æ–‡æœ¬å’Œé¡µç ä¿¡æ¯
text, page_numbers = extract_text_with_page_numbers(pdf_reader)
text


print(f"æå–çš„æ–‡æœ¬é•¿åº¦: {len(text)} ä¸ªå­—ç¬¦ã€‚")
    
# å¤„ç†æ–‡æœ¬å¹¶åˆ›å»ºçŸ¥è¯†åº“ï¼ŒåŒæ—¶ä¿å­˜åˆ°ç£ç›˜
save_dir = os.path.join(SCRIPT_DIR, "vector_db")
knowledgeBase = process_text_with_splitter(text, page_numbers, save_path=save_dir)

# ç¤ºä¾‹ï¼šå¦‚ä½•åŠ è½½å·²ä¿å­˜çš„å‘é‡æ•°æ®åº“
# æ³¨é‡Šæ‰ä»¥ä¸‹ä»£ç ä»¥é¿å…åœ¨å½“å‰è¿è¡Œä¸­é‡å¤åŠ è½½
"""
# åˆ›å»ºåµŒå…¥æ¨¡å‹
embeddings = DashScopeEmbeddings(
    model="text-embedding-v1",
    dashscope_api_key=DASHSCOPE_API_KEY,
)
# ä»ç£ç›˜åŠ è½½å‘é‡æ•°æ®åº“
loaded_knowledgeBase = load_knowledge_base("./vector_db", embeddings)
# ä½¿ç”¨åŠ è½½çš„çŸ¥è¯†åº“è¿›è¡ŒæŸ¥è¯¢
docs = loaded_knowledgeBase.similarity_search("å®¢æˆ·ç»ç†æ¯å¹´è¯„è˜ç”³æŠ¥æ—¶é—´æ˜¯æ€æ ·çš„ï¼Ÿ")

# ç›´æ¥ä½¿ç”¨FAISS.load_localæ–¹æ³•åŠ è½½ï¼ˆæ›¿ä»£æ–¹æ³•ï¼‰
# loaded_knowledgeBase = FAISS.load_local("./vector_db", embeddings, allow_dangerous_deserialization=True)
# æ³¨æ„ï¼šä½¿ç”¨è¿™ç§æ–¹æ³•åŠ è½½æ—¶ï¼Œéœ€è¦æ‰‹åŠ¨åŠ è½½é¡µç ä¿¡æ¯
"""

from langchain_community.llms import Tongyi
llm = Tongyi(model_name="deepseek-v3", dashscope_api_key=DASHSCOPE_API_KEY) # qwen-turbo

# è®¾ç½®æŸ¥è¯¢é—®é¢˜
query = "å®¢æˆ·ç»ç†è¢«æŠ•è¯‰äº†ï¼ŒæŠ•è¯‰ä¸€æ¬¡æ‰£å¤šå°‘åˆ†"
#query = "å®¢æˆ·ç»ç†æ¯å¹´è¯„è˜ç”³æŠ¥æ—¶é—´æ˜¯æ€æ ·çš„ï¼Ÿ"
if query:
    # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢ï¼Œæ‰¾åˆ°ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£
    docs = knowledgeBase.similarity_search(query,k=2)

    # åŠ è½½é—®ç­”é“¾
    chain = load_qa_chain(llm, chain_type="stuff")

    # å‡†å¤‡è¾“å…¥æ•°æ®
    input_data = {"input_documents": docs, "question": query}

    # ä½¿ç”¨å›è°ƒå‡½æ•°è·Ÿè¸ªAPIè°ƒç”¨æˆæœ¬
    with get_openai_callback() as cost:
        # æ‰§è¡Œé—®ç­”é“¾
        response = chain.invoke(input=input_data)
        print(f"æŸ¥è¯¢å·²å¤„ç†ã€‚æˆæœ¬: {cost}")
        print(response["output_text"])
        print("æ¥æº:")

    # è®°å½•å”¯ä¸€çš„é¡µç 
    unique_pages = set()

    # æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£å—çš„æ¥æºé¡µç 
    for doc in docs:
        text_content = getattr(doc, "page_content", "")
        source_page = knowledgeBase.page_info.get(
            text_content.strip(), "æœªçŸ¥"
        )

        if source_page not in unique_pages:
            unique_pages.add(source_page)
            print(f"æ–‡æœ¬å—é¡µç : {source_page}")


import pickle
with open(os.path.join(SCRIPT_DIR, "vector_db/page_info.pkl"), "rb") as f:
    page_info = pickle.load(f)

print("\n" + "=" * 60)
print("ğŸ“„ page_info.pkl å†…å®¹é¢„è§ˆ")
print("=" * 60)
for i, (chunk, page) in enumerate(page_info.items(), 1):
    print(f"\n{'â”€' * 60}")
    print(f"ğŸ“Œ æ–‡æœ¬å— {i} | é¡µç : {page}")
    print(f"{'â”€' * 60}")
    print(f"{chunk[:200]}{'...' if len(chunk) > 200 else ''}")
print(f"\n{'=' * 60}")
print(f"ğŸ“Š å…± {len(page_info)} ä¸ªæ–‡æœ¬å—")
print("=" * 60)