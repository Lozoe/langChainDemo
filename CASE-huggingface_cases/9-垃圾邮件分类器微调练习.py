"""
åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨å¾®è°ƒç»ƒä¹ 
ä½¿ç”¨BERTæ¨¡å‹è¿›è¡Œåƒåœ¾é‚®ä»¶åˆ†ç±»ï¼ŒæŒæ¡Huggingfaceæ¨¡å‹å¾®è°ƒçš„å®Œæ•´æµç¨‹

æ­¥éª¤ï¼š
1. å‡†å¤‡ç¯å¢ƒä¸æ•°æ®
2. æ•°æ®é¢„å¤„ç†ï¼ˆTokenizationï¼‰
3. DataCollator åŠ¨æ€è¡¥é½
4. æ¨¡å‹åŠ è½½
5. è¯„ä¼°æŒ‡æ ‡å®šä¹‰
6. è®­ç»ƒé…ç½®ä¸æ‰§è¡Œ
7. æ¨¡å‹æ¨ç†
"""

# ==================== Step 1: å‡†å¤‡ç¯å¢ƒä¸æ•°æ® ====================
# å®‰è£…ä¾èµ–ï¼ˆå¦‚æœªå®‰è£…è¯·å–æ¶ˆæ³¨é‡Šè¿è¡Œï¼‰
# !pip install transformers datasets evaluate accelerate scikit-learn

import os
import numpy as np
from datasets import load_dataset, Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
)
import evaluate

# è®¾ç½®éšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°
SEED = 42
np.random.seed(SEED)

# åŠ è½½SMSåƒåœ¾é‚®ä»¶æ•°æ®é›†
# æ•°æ®é›†åŒ…å«çŸ­ä¿¡å†…å®¹å’Œæ ‡ç­¾ï¼ˆham=æ­£å¸¸é‚®ä»¶, spam=åƒåœ¾é‚®ä»¶ï¼‰
print("=" * 50)
print("Step 1: åŠ è½½æ•°æ®é›†")
print("=" * 50)

# ä½¿ç”¨Huggingfaceçš„SMS Spamæ•°æ®é›†
dataset = load_dataset("sms_spam")
print(f"æ•°æ®é›†ä¿¡æ¯: {dataset}")
print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(dataset['train'])}")

# æŸ¥çœ‹æ•°æ®æ ·ä¾‹
print("\næ•°æ®æ ·ä¾‹:")
for i in range(3):
    sample = dataset['train'][i]
    print(f"  [{i}] æ ‡ç­¾: {sample['label']} | å†…å®¹: {sample['sms'][:50]}...")

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
dataset = dataset['train'].train_test_split(test_size=0.2, seed=SEED)
print(f"\nåˆ’åˆ†å - è®­ç»ƒé›†: {len(dataset['train'])} | æµ‹è¯•é›†: {len(dataset['test'])}")

# å®šä¹‰æ ‡ç­¾æ˜ å°„
label2id = {"ham": 0, "spam": 1}
id2label = {0: "ham", 1: "spam"}


# ==================== Step 2: æ•°æ®é¢„å¤„ç†ï¼ˆTokenizationï¼‰ ====================
print("\n" + "=" * 50)
print("Step 2: æ•°æ®é¢„å¤„ç†ï¼ˆTokenizationï¼‰")
print("=" * 50)

# é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨bert-base-uncasedï¼‰
MODEL_NAME = "bert-base-uncased"
# å¦‚æœç½‘ç»œé—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨å›½å†…é•œåƒæˆ–æœ¬åœ°æ¨¡å‹
# MODEL_NAME = "hfl/chinese-bert-wwm-ext"  # ä¸­æ–‡BERT

# åŠ è½½Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
print(f"TokenizeråŠ è½½å®Œæˆ: {MODEL_NAME}")

# å®šä¹‰tokenizeå‡½æ•°
def tokenize_function(examples):
    """
    å¯¹æ–‡æœ¬è¿›è¡Œtokenization
    - truncation=True: è¶…è¿‡æœ€å¤§é•¿åº¦æ—¶æˆªæ–­
    - max_length: è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
    """
    return tokenizer(
        examples["sms"],
        truncation=True,
        max_length=128,
        # paddingåœ¨è¿™é‡Œä¸åšï¼Œç”±DataCollatoråŠ¨æ€è¡¥é½
    )

# å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œtokenization
tokenized_datasets = dataset.map(
    tokenize_function,
    batched=True,  # æ‰¹é‡å¤„ç†ï¼Œæé«˜æ•ˆç‡
    remove_columns=["sms"],  # ç§»é™¤åŸå§‹æ–‡æœ¬åˆ—
)

print(f"Tokenizationå®Œæˆ!")
print(f"å¤„ç†åçš„ç‰¹å¾: {tokenized_datasets['train'].features}")

# æŸ¥çœ‹tokenizationç»“æœ
print("\nTokenizationæ ·ä¾‹:")
sample = tokenized_datasets['train'][0]
print(f"  input_idsé•¿åº¦: {len(sample['input_ids'])}")
print(f"  input_idså‰20ä¸ª: {sample['input_ids'][:20]}")
print(f"  attention_maskå‰20ä¸ª: {sample['attention_mask'][:20]}")


# ==================== Step 3: DataCollator åŠ¨æ€è¡¥é½ ====================
print("\n" + "=" * 50)
print("Step 3: DataCollator åŠ¨æ€è¡¥é½")
print("=" * 50)

# åˆ›å»ºDataCollator
# DataCollatorWithPaddingä¼šåœ¨æ¯ä¸ªbatchä¸­åŠ¨æ€è¡¥é½åˆ°è¯¥batchçš„æœ€å¤§é•¿åº¦
# è¿™æ¯”é¢„å…ˆpaddingåˆ°å›ºå®šé•¿åº¦æ›´é«˜æ•ˆ
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

print("DataCollatoråˆ›å»ºå®Œæˆ!")
print("  - ç±»å‹: DataCollatorWithPadding")
print("  - åŠŸèƒ½: åŠ¨æ€è¡¥é½æ¯ä¸ªbatchåˆ°è¯¥batchçš„æœ€å¤§é•¿åº¦")
print("  - ä¼˜åŠ¿: å‡å°‘ä¸å¿…è¦çš„paddingï¼Œæé«˜è®­ç»ƒæ•ˆç‡")


# ==================== Step 4: æ¨¡å‹åŠ è½½ ====================
print("\n" + "=" * 50)
print("Step 4: æ¨¡å‹åŠ è½½")
print("=" * 50)

# åŠ è½½é¢„è®­ç»ƒBERTæ¨¡å‹ç”¨äºåºåˆ—åˆ†ç±»
# num_labels=2 è¡¨ç¤ºäºŒåˆ†ç±»ä»»åŠ¡ï¼ˆham/spamï¼‰
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=2,
    id2label=id2label,
    label2id=label2id,
)

print(f"æ¨¡å‹åŠ è½½å®Œæˆ: {MODEL_NAME}")
print(f"  - åˆ†ç±»ç±»åˆ«æ•°: 2")
print(f"  - æ ‡ç­¾æ˜ å°„: {id2label}")
print(f"  - æ¨¡å‹å‚æ•°é‡: {model.num_parameters():,}")


# ==================== Step 5: è¯„ä¼°æŒ‡æ ‡å®šä¹‰ ====================
print("\n" + "=" * 50)
print("Step 5: è¯„ä¼°æŒ‡æ ‡å®šä¹‰")
print("=" * 50)

# åŠ è½½è¯„ä¼°æŒ‡æ ‡
accuracy_metric = evaluate.load("accuracy")
precision_metric = evaluate.load("precision")
recall_metric = evaluate.load("recall")
f1_metric = evaluate.load("f1")

def compute_metrics(eval_pred):
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    - accuracy: å‡†ç¡®ç‡
    - precision: ç²¾ç¡®ç‡
    - recall: å¬å›ç‡
    - f1: F1åˆ†æ•°
    """
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)
    precision = precision_metric.compute(predictions=predictions, references=labels, average="binary")
    recall = recall_metric.compute(predictions=predictions, references=labels, average="binary")
    f1 = f1_metric.compute(predictions=predictions, references=labels, average="binary")
    
    return {
        "accuracy": accuracy["accuracy"],
        "precision": precision["precision"],
        "recall": recall["recall"],
        "f1": f1["f1"],
    }

print("è¯„ä¼°æŒ‡æ ‡å®šä¹‰å®Œæˆ!")
print("  - accuracy: å‡†ç¡®ç‡ = æ­£ç¡®é¢„æµ‹æ•° / æ€»é¢„æµ‹æ•°")
print("  - precision: ç²¾ç¡®ç‡ = TP / (TP + FP)")
print("  - recall: å¬å›ç‡ = TP / (TP + FN)")
print("  - f1: F1åˆ†æ•° = 2 * precision * recall / (precision + recall)")


# ==================== Step 6: è®­ç»ƒé…ç½®ä¸æ‰§è¡Œ ====================
print("\n" + "=" * 50)
print("Step 6: è®­ç»ƒé…ç½®ä¸æ‰§è¡Œ")
print("=" * 50)

# å®šä¹‰è¾“å‡ºç›®å½•
OUTPUT_DIR = "./spam_classifier_output"

# é…ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,                    # è¾“å‡ºç›®å½•
    eval_strategy="epoch",                    # æ¯ä¸ªepochè¯„ä¼°ä¸€æ¬¡
    save_strategy="epoch",                    # æ¯ä¸ªepochä¿å­˜ä¸€æ¬¡
    learning_rate=2e-5,                       # å­¦ä¹ ç‡
    per_device_train_batch_size=16,           # è®­ç»ƒbatchå¤§å°
    per_device_eval_batch_size=16,            # è¯„ä¼°batchå¤§å°
    num_train_epochs=3,                       # è®­ç»ƒè½®æ•°
    weight_decay=0.01,                        # æƒé‡è¡°å‡
    load_best_model_at_end=True,              # è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€ä½³æ¨¡å‹
    metric_for_best_model="f1",               # ç”¨äºé€‰æ‹©æœ€ä½³æ¨¡å‹çš„æŒ‡æ ‡
    logging_dir=f"{OUTPUT_DIR}/logs",         # æ—¥å¿—ç›®å½•
    logging_steps=50,                         # æ¯50æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    seed=SEED,                                # éšæœºç§å­
    # å¦‚æœæœ‰GPUï¼Œå¯ä»¥å¯ç”¨ä»¥ä¸‹é…ç½®
    # fp16=True,                              # æ··åˆç²¾åº¦è®­ç»ƒ
)

print("è®­ç»ƒå‚æ•°é…ç½®:")
print(f"  - è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
print(f"  - å­¦ä¹ ç‡: {training_args.learning_rate}")
print(f"  - è®­ç»ƒè½®æ•°: {training_args.num_train_epochs}")
print(f"  - Batchå¤§å°: {training_args.per_device_train_batch_size}")
print(f"  - æƒé‡è¡°å‡: {training_args.weight_decay}")

# åˆ›å»ºTrainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

print("\nTraineråˆ›å»ºå®Œæˆï¼Œå¼€å§‹è®­ç»ƒ...")
print("-" * 50)

# å¼€å§‹è®­ç»ƒ
train_result = trainer.train()

# ä¿å­˜æ¨¡å‹
trainer.save_model()
print(f"\næ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")

# è¾“å‡ºè®­ç»ƒç»“æœ
print("\nè®­ç»ƒç»“æœ:")
print(f"  - è®­ç»ƒæŸå¤±: {train_result.training_loss:.4f}")
print(f"  - è®­ç»ƒæ—¶é—´: {train_result.metrics['train_runtime']:.2f}ç§’")

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
print("\n" + "-" * 50)
print("åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
eval_results = trainer.evaluate()
print("\nè¯„ä¼°ç»“æœ:")
for key, value in eval_results.items():
    if isinstance(value, float):
        print(f"  - {key}: {value:.4f}")


# ==================== Step 7: æ¨¡å‹æ¨ç† ====================
print("\n" + "=" * 50)
print("Step 7: æ¨¡å‹æ¨ç†")
print("=" * 50)

from transformers import pipeline

# æ–¹æ³•1: ä½¿ç”¨pipelineè¿›è¡Œæ¨ç†ï¼ˆæ¨èï¼Œç®€å•æ˜“ç”¨ï¼‰
print("\næ–¹æ³•1: ä½¿ç”¨Pipelineæ¨ç†")
print("-" * 30)

# åˆ›å»ºåˆ†ç±»pipeline
classifier = pipeline(
    "text-classification",
    model=trainer.model,
    tokenizer=tokenizer,
)

# æµ‹è¯•æ ·ä¾‹
test_texts = [
    "Congratulations! You've won a free iPhone! Click here to claim your prize!",
    "Hi, are we still meeting for lunch tomorrow at 12pm?",
    "URGENT: Your account has been compromised. Click here immediately!",
    "Hey, just wanted to check if you got my email about the project.",
    "FREE MONEY! Get $1000 cash now! Limited time offer!",
]

print("æ¨ç†ç»“æœ:")
for text in test_texts:
    result = classifier(text)[0]
    label = result['label']
    score = result['score']
    emoji = "ğŸš«" if label == "spam" else "âœ…"
    print(f"  {emoji} [{label}] (ç½®ä¿¡åº¦: {score:.4f})")
    print(f"     å†…å®¹: {text[:50]}...")
    print()


# æ–¹æ³•2: æ‰‹åŠ¨æ¨ç†ï¼ˆæ›´çµæ´»ï¼‰
print("\næ–¹æ³•2: æ‰‹åŠ¨æ¨ç†")
print("-" * 30)

import torch

def predict(text, model, tokenizer):
    """
    æ‰‹åŠ¨è¿›è¡Œæ¨¡å‹æ¨ç†
    """
    # Tokenizeè¾“å…¥
    inputs = tokenizer(
        text,
        truncation=True,
        max_length=128,
        return_tensors="pt",
    )
    
    # ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # æ¨ç†
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probabilities = torch.softmax(logits, dim=-1)
        predicted_class = torch.argmax(probabilities, dim=-1).item()
        confidence = probabilities[0][predicted_class].item()
    
    return {
        "label": id2label[predicted_class],
        "confidence": confidence,
        "probabilities": {
            "ham": probabilities[0][0].item(),
            "spam": probabilities[0][1].item(),
        }
    }

# æµ‹è¯•æ‰‹åŠ¨æ¨ç†
test_text = "Win a FREE vacation to Hawaii! Reply YES now!"
result = predict(test_text, trainer.model, tokenizer)
print(f"æµ‹è¯•æ–‡æœ¬: {test_text}")
print(f"é¢„æµ‹ç»“æœ: {result['label']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.4f}")
print(f"æ¦‚ç‡åˆ†å¸ƒ: ham={result['probabilities']['ham']:.4f}, spam={result['probabilities']['spam']:.4f}")


# ==================== æ€»ç»“ ====================
print("\n" + "=" * 50)
print("ğŸ‰ åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨å¾®è°ƒå®Œæˆï¼")
print("=" * 50)
print("""
æœ¬ç»ƒä¹ å®Œæˆäº†ä»¥ä¸‹æ­¥éª¤:

1. âœ… å‡†å¤‡ç¯å¢ƒä¸æ•°æ®
   - åŠ è½½SMS Spamæ•°æ®é›†
   - åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†

2. âœ… æ•°æ®é¢„å¤„ç†ï¼ˆTokenizationï¼‰
   - ä½¿ç”¨BERT tokenizerå¤„ç†æ–‡æœ¬
   - è½¬æ¢ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ¼å¼

3. âœ… DataCollator åŠ¨æ€è¡¥é½
   - ä½¿ç”¨DataCollatorWithPaddingåŠ¨æ€è¡¥é½

4. âœ… æ¨¡å‹åŠ è½½
   - åŠ è½½é¢„è®­ç»ƒBERTæ¨¡å‹
   - é…ç½®äºŒåˆ†ç±»ä»»åŠ¡

5. âœ… è¯„ä¼°æŒ‡æ ‡å®šä¹‰
   - å®šä¹‰accuracy, precision, recall, f1

6. âœ… è®­ç»ƒé…ç½®ä¸æ‰§è¡Œ
   - é…ç½®è®­ç»ƒå‚æ•°
   - ä½¿ç”¨Trainerè¿›è¡Œå¾®è°ƒ

7. âœ… æ¨¡å‹æ¨ç†
   - Pipelineæ–¹å¼æ¨ç†
   - æ‰‹åŠ¨æ¨ç†æ–¹å¼

æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}
""".format(OUTPUT_DIR=OUTPUT_DIR))
