{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d09c680-1314-4553-9006-1deca3f701ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 00:01:58.004456: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-04 00:01:58.016451: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770134518.031994   88131 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770134518.036605   88131 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770134518.049224   88131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770134518.049242   88131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770134518.049243   88131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770134518.049245   88131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-04 00:01:58.053757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从 ModelScope 下载模型: Qwen/Qwen2.5-7B-Instruct\n",
      "模型将保存到: /root/autodl-tmp/models\n",
      "提示：首次运行会下载模型，请耐心等待...\n",
      "Downloading Model from https://www.modelscope.cn to directory: /root/autodl-tmp/models/Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 00:02:00,266 - modelscope - INFO - Got 1 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc25220de17460783a6f94aeb0b56de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 1 items:   0%|          | 0.00/1.00 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540ad35066bf4d1dae7d74342320c8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/6.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 00:02:00,743 - modelscope - INFO - Download model 'Qwen/Qwen2.5-7B-Instruct' successfully.\n",
      "2026-02-04 00:02:00,746 - modelscope - INFO - Creating symbolic link [/root/autodl-tmp/models/Qwen/Qwen2.5-7B-Instruct].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已下载到: /root/autodl-tmp/models/Qwen/Qwen2___5-7B-Instruct\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## 使用 Qwen 大模型进行垃圾邮件分类（无需微调）\n",
    "# 使用 ModelScope 下载模型，然后用 HuggingFace 加载和使用\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from modelscope import snapshot_download  # 使用 ModelScope 下载模型\n",
    "\n",
    "\n",
    "# ## 使用 ModelScope 下载模型，然后用 HuggingFace 加载\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Step 1: 使用 ModelScope 从魔搭社区下载 Qwen3-0.6B 模型\n",
    "# ModelScope 是下载工具，HuggingFace 是加载工具\n",
    "modelscope_model_id = \"Qwen/Qwen2.5-7B-Instruct\"  # ModelScope 上的模型 ID\n",
    "cache_dir = \"/root/autodl-tmp/models\"  # 指定模型下载和缓存的文件夹\n",
    "\n",
    "print(f\"正在从 ModelScope 下载模型: {modelscope_model_id}\")\n",
    "print(f\"模型将保存到: {cache_dir}\")\n",
    "print(\"提示：首次运行会下载模型，请耐心等待...\")\n",
    "\n",
    "try:\n",
    "    # 从 ModelScope 下载模型文件到指定文件夹\n",
    "    model_dir = snapshot_download(\n",
    "        modelscope_model_id,\n",
    "        cache_dir=cache_dir  # 指定下载文件夹\n",
    "    )\n",
    "    print(f\"模型已下载到: {model_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"ModelScope 下载失败: {e}\")\n",
    "    print(\"提示：请确保已安装 modelscope: pip install modelscope\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dddfa35d-7c76-47d5-843d-25857f5be3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在使用 HuggingFace 加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到 GPU: NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e944c23dba3146d2bedd215ef14ed789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成！\n"
     ]
    }
   ],
   "source": [
    "# Step 1: 使用 HuggingFace 加载本地模型\n",
    "print(\"\\n正在使用 HuggingFace 加载模型...\")\n",
    "\n",
    "try:\n",
    "    # 使用本地路径加载 tokenizer 和 model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    \n",
    "    # 检查是否有 GPU\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"检测到 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir,  # 使用本地路径\n",
    "            torch_dtype=torch.float16,  # 使用半精度节省显存\n",
    "            device_map=\"auto\",  # 自动分配设备\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"未检测到 GPU，使用 CPU（速度较慢）\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir,  # 使用本地路径\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    print(\"模型加载完成！\")\n",
    "except Exception as e:\n",
    "    print(f\"模型加载失败: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea2da1c-68fd-4be0-8e3b-28fc417fb545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "方法1：直接使用模型 + Prompt\n",
      "============================================================\n",
      "文本: 今晚有空一起吃饭吗？\n",
      "预测: 正常邮件 (ID: 0)\n",
      "------------------------------------------------------------\n",
      "文本: 恭喜您获得500万大奖，点击领取\n",
      "预测: 垃圾邮件 (ID: 1)\n",
      "------------------------------------------------------------\n",
      "文本: 您的验证码是1234，请勿泄露\n",
      "预测: 垃圾邮件 (ID: 1)\n",
      "------------------------------------------------------------\n",
      "文本: 澳门首家线上赌场上线啦\n",
      "预测: 垃圾邮件 (ID: 1)\n",
      "------------------------------------------------------------\n",
      "文本: 项目进度怎么样了？需不需要开会\n",
      "预测: 正常邮件 (ID: 0)\n",
      "------------------------------------------------------------\n",
      "文本: 独家内幕消息，股票必涨，加群\n",
      "预测: 垃圾邮件 (ID: 1)\n",
      "------------------------------------------------------------\n",
      "文本: 低息贷款，无抵押，秒到账\n",
      "预测: 垃圾邮件 (ID: 1)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ## Step2：定义分类函数（使用 Prompt）\n",
    "\n",
    "def classify_spam_with_prompt(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    使用 Prompt 让大模型进行分类\n",
    "    \"\"\"\n",
    "    # 构建分类提示词（使用更清晰的格式）\n",
    "    prompt = f\"\"\"你是一个垃圾邮件分类专家。请判断以下文本是否为垃圾邮件。\n",
    "\n",
    "文本：{text}\n",
    "\n",
    "请只回答\"垃圾邮件\"或\"正常邮件\"：\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to(model.device)\n",
    "    \n",
    "    # 生成回答\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=15,  # 生成少量token即可\n",
    "            do_sample=False,    # 使用贪心解码，保证结果稳定\n",
    "            pad_token_id=tokenizer.eos_token_id,  # 设置pad token\n",
    "        )\n",
    "    \n",
    "    # 解码输出（只取新生成的部分）\n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    \n",
    "    # 解析结果\n",
    "    generated_text_lower = generated_text.lower()\n",
    "    #print('generated_text_lower=', generated_text_lower)\n",
    "    if \"垃圾\" in generated_text or \"spam\" in generated_text_lower:\n",
    "        return 1, \"垃圾邮件\"\n",
    "    elif \"正常\" in generated_text or \"normal\" in generated_text_lower or \"非垃圾\" in generated_text:\n",
    "        return 0, \"正常邮件\"\n",
    "    else:\n",
    "        # 如果模型输出不符合预期，返回原始输出用于调试\n",
    "        return None, f\"未识别: {generated_text}\"\n",
    "\n",
    "\n",
    "# ## Step3：使用 Pipeline 方式（更简单）\n",
    "def classify_spam_with_pipeline(text, generator):\n",
    "    \"\"\"\n",
    "    使用 Pipeline 进行分类（更简单的方式）\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"你是一个垃圾邮件分类专家。请判断以下文本是否为垃圾邮件。\n",
    "\n",
    "文本：{text}\n",
    "\n",
    "请只回答\"垃圾邮件\"或\"正常邮件\"：\"\"\"\n",
    "    \n",
    "    # 使用 pipeline 生成\n",
    "    try:\n",
    "        result = generator(\n",
    "            prompt,\n",
    "            max_new_tokens=15,\n",
    "            do_sample=False,\n",
    "            temperature=0.1,\n",
    "            return_full_text=False,  # 只返回新生成的部分\n",
    "            pad_token_id=generator.tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        generated_text = result[0]['generated_text'].strip()\n",
    "        \n",
    "        # 解析结果\n",
    "        generated_text_lower = generated_text.lower()\n",
    "        #print('generated_text_lower=', generated_text_lower)\n",
    "        if \"垃圾\" in generated_text or \"spam\" in generated_text_lower:\n",
    "            return 1, \"垃圾邮件\"\n",
    "        elif \"正常\" in generated_text or \"normal\" in generated_text_lower or \"非垃圾\" in generated_text:\n",
    "            return 0, \"正常邮件\"\n",
    "        else:\n",
    "            return None, f\"未识别: {generated_text}\"\n",
    "    except Exception as e:\n",
    "        return None, f\"错误: {str(e)}\"\n",
    "\n",
    "\n",
    "# ## Step4：测试分类效果\n",
    "# 准备测试数据\n",
    "test_texts = [\n",
    "    \"今晚有空一起吃饭吗？\",           # 正常\n",
    "    \"恭喜您获得500万大奖，点击领取\",   # 垃圾\n",
    "    \"您的验证码是1234，请勿泄露\",      # 正常\n",
    "    \"澳门首家线上赌场上线啦\",          # 垃圾\n",
    "    \"项目进度怎么样了？需不需要开会\",   # 正常\n",
    "    \"独家内幕消息，股票必涨，加群\",     # 垃圾\n",
    "    \"低息贷款，无抵押，秒到账\",        # 垃圾\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"方法1：直接使用模型 + Prompt\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_texts:\n",
    "    label_id, label_name = classify_spam_with_prompt(text, model, tokenizer)\n",
    "    print(f\"文本: {text}\")\n",
    "    print(f\"预测: {label_name} (ID: {label_id})\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d157ae12-e431-42cd-b76c-8c81a6850958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "方法2：使用 Pipeline（推荐，更简单）\n",
      "============================================================\n",
      "Pipeline 创建成功！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本: 今晚有空一起吃饭吗？\n",
      "预测: 正常邮件 (ID: 0)\n",
      "------------------------------------------------------------\n",
      "文本: 恭喜您获得500万大奖，点击领取\n",
      "预测: 垃圾邮件 (ID: 1)\n",
      "------------------------------------------------------------\n",
      "文本: 您的验证码是1234，请勿泄露\n",
      "预测: 垃圾邮件 (ID: 1)\n",
      "------------------------------------------------------------\n",
      "文本: 澳门首家线上赌场上线啦\n",
      "预测: 垃圾邮件 (ID: 1)\n",
      "------------------------------------------------------------\n",
      "文本: 项目进度怎么样了？需不需要开会\n",
      "预测: 正常邮件 (ID: 0)\n",
      "------------------------------------------------------------\n",
      "文本: 独家内幕消息，股票必涨，加群\n",
      "预测: 垃圾邮件 (ID: 1)\n",
      "------------------------------------------------------------\n",
      "文本: 低息贷款，无抵押，秒到账\n",
      "预测: 垃圾邮件 (ID: 1)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"方法2：使用 Pipeline（推荐，更简单）\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 创建 text-generation pipeline\n",
    "# 注意：如果模型使用了 device_map=\"auto\"，模型已经通过 accelerate 分配到设备\n",
    "# 此时不能指定 device 参数，Pipeline 会自动检测模型所在的设备\n",
    "try:\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "        # 不指定 device，让 Pipeline 自动检测模型所在的设备\n",
    "    )\n",
    "    print(\"Pipeline 创建成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"Pipeline 创建失败: {e}\")\n",
    "    generator = None\n",
    "\n",
    "if generator is not None:\n",
    "    for text in test_texts:\n",
    "        label_id, label_name = classify_spam_with_pipeline(text, generator)\n",
    "        print(f\"文本: {text}\")\n",
    "        print(f\"预测: {label_name} (ID: {label_id})\")\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"Pipeline 未创建，跳过 Pipeline 方式测试\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ef44656-1b3e-44ba-995e-dfa9e2a9305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "批量处理示例\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常邮件: 今晚有空一起吃饭吗？\n",
      "垃圾邮件: 恭喜您获得500万大奖，点击领取\n",
      "垃圾邮件: 您的验证码是1234，请勿泄露\n",
      "垃圾邮件: 澳门首家线上赌场上线啦\n",
      "正常邮件: 项目进度怎么样了？需不需要开会\n",
      "垃圾邮件: 独家内幕消息，股票必涨，加群\n",
      "垃圾邮件: 低息贷款，无抵押，秒到账\n"
     ]
    }
   ],
   "source": [
    "# ## Step5：批量处理示例\n",
    "\n",
    "def batch_classify(texts, generator):\n",
    "    \"\"\"\n",
    "    批量分类（使用 Pipeline）\n",
    "    \"\"\"\n",
    "    if generator is None:\n",
    "        print(\"Pipeline 未创建，无法批量处理\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    for text in texts:\n",
    "        label_id, label_name = classify_spam_with_pipeline(text, generator)\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"label\": label_id,\n",
    "            \"label_name\": label_name\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# 批量处理\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"批量处理示例\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if generator is not None:\n",
    "    batch_results = batch_classify(test_texts, generator)\n",
    "    for result in batch_results:\n",
    "        print(f\"{result['label_name']}: {result['text']}\")\n",
    "else:\n",
    "    print(\"无法执行批量处理（Pipeline 未创建）\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
